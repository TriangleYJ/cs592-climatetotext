{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f64ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas meteostat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d2ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from meteostat import Stations, Hourly\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "# 경고 메시지 무시\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"스크립트를 시작합니다...\")\n",
    "\n",
    "# --- 1. 설정값 ---\n",
    "START_DATE = datetime(2023, 1, 1)\n",
    "END_DATE = datetime(2023, 12, 31, 23, 59)\n",
    "\n",
    "# 전 세계 관측소 중 몇 개를 무작위로 테스트할지 결정\n",
    "N_STATION_SAMPLES = 10000\n",
    "\n",
    "# (변경됨) 목표로 하는 전체 데이터셋 크기\n",
    "TOTAL_DATASET_SIZE_N = 1000000\n",
    "\n",
    "# (추가됨) 로컬 시간대 기준으로 필터링할 시간대 (10, 11, 13, 14시)\n",
    "TARGET_LOCAL_HOURS = [10, 11, 13, 14, 22, 23, 1, 2]\n",
    "\n",
    "OUTPUT_FILE = \"gee_weather_ground_truth_balanced.csv\"\n",
    "\n",
    "# --- 2. 전 세계 관측소 목록 가져오기 ---\n",
    "print(f\"전 세계 {START_DATE.year}년 기준 활성 관측소를 가져옵니다...\")\n",
    "stations = Stations()\n",
    "stations = stations.fetch()\n",
    "\n",
    "active_stations = stations[stations['hourly_end'] >= END_DATE]\n",
    "print(f\"활성 관측소 {len(active_stations)}개 확인.\")\n",
    "\n",
    "n_to_sample = min(len(active_stations), N_STATION_SAMPLES)\n",
    "station_sample = active_stations.sample(n=n_to_sample, random_state=42)\n",
    "print(f\"이 중 {n_to_sample}개 관측소를 샘플링하여 데이터를 수집합니다...\")\n",
    "\n",
    "# --- 3. 경도 기반 시간대 오프셋 계산 함수 ---\n",
    "def get_utc_offset_from_lon(lon):\n",
    "    \"\"\"\n",
    "    경도를 기반으로 대략적인 UTC 오프셋을 시간 단위로 계산합니다.\n",
    "    경도 15도당 1시간씩 변화합니다.\n",
    "    \"\"\"\n",
    "    return int(round(lon / 15.0))\n",
    "\n",
    "# --- 4. 데이터 수집 및 필터링 ---\n",
    "all_weather_events = []\n",
    "\n",
    "for index, station in tqdm(station_sample.iterrows(), total=n_to_sample, desc=\"관측소 데이터 수집 중\"):\n",
    "    station_id = station.name\n",
    "    lat = station['latitude']\n",
    "    lon = station['longitude']\n",
    "    \n",
    "    # 경도 기반 UTC 오프셋 계산\n",
    "    utc_offset_hours = get_utc_offset_from_lon(lon)\n",
    "\n",
    "    try:\n",
    "        data = Hourly(station_id, START_DATE, END_DATE)\n",
    "        data = data.fetch()\n",
    "\n",
    "        if data.empty:\n",
    "            continue\n",
    "\n",
    "        # (변경됨) 날씨 코드(coco)가 0보다 큰 경우 (1:맑음 ~ 27:폭풍)\n",
    "        events = data[data['coco'] > 0] \n",
    "\n",
    "        if not events.empty:\n",
    "            events = events.reset_index()\n",
    "            \n",
    "            # UTC 시간을 로컬 시간으로 변환\n",
    "            events['local_time'] = events['time'].apply(lambda x: x + timedelta(hours=utc_offset_hours))\n",
    "            events['local_hour'] = events['local_time'].dt.hour\n",
    "            \n",
    "            # 로컬 시간대 기준으로 10, 11, 13, 14시만 필터링\n",
    "            events = events[events['local_hour'].isin(TARGET_LOCAL_HOURS)]\n",
    "            \n",
    "            if not events.empty:\n",
    "                events['id'] = station_id\n",
    "                events['lat'] = lat\n",
    "                events['lon'] = lon\n",
    "                events['time'] = events['time'].dt.tz_localize(None)\n",
    "                # 모든 기상 데이터 컬럼 포함\n",
    "                all_weather_events.append(events)\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "if not all_weather_events:\n",
    "    print(\"오류: 수집된 기상 현상 데이터가 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "# --- 5. 데이터 통합 및 정제 ---\n",
    "print(\"\\n데이터 통합 및 정제 중...\")\n",
    "final_df = pd.concat(all_weather_events, ignore_index=True)\n",
    "final_df.dropna(subset=['coco'], inplace=True)\n",
    "final_df['coco'] = final_df['coco'].astype(int)\n",
    "\n",
    "# 컬럼 순서 재배열 (주요 컬럼을 앞으로)\n",
    "main_cols = ['id', 'time', 'lat', 'lon', 'coco', 'local_hour']\n",
    "other_cols = [col for col in final_df.columns if col not in main_cols and col != 'local_time']\n",
    "final_df = final_df[main_cols + other_cols]\n",
    "\n",
    "print(f\"총 {len(final_df)}개의 기상 현상 이벤트를 수집했습니다. (로컬 시간 {TARGET_LOCAL_HOURS}시 필터링 적용)\")\n",
    "print(f\"저장되는 컬럼: {list(final_df.columns)}\")\n",
    "print(\"\\n수집된 데이터의 날씨 코드(coco) 분포 (상위 10개):\")\n",
    "print(final_df['coco'].value_counts().head(10))\n",
    "print(\"\\n로컬 시간대별 데이터 분포:\")\n",
    "print(final_df['local_hour'].value_counts().sort_index())\n",
    "\n",
    "# --- 6. 날씨 코드별 균형 샘플링 (Stratified Sampling) ---\n",
    "print(f\"\\n목표 크기 {TOTAL_DATASET_SIZE_N}에 맞춰 균형 샘플링을 수행합니다...\")\n",
    "\n",
    "# (변경됨) 날씨 코드 종류 수 자동 계산\n",
    "weather_type_pool_size = final_df['coco'].nunique()\n",
    "\n",
    "if weather_type_pool_size == 0:\n",
    "    print(\"오류: 유니크한 날씨 코드를 찾을 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "# (변경됨) 클래스별 샘플링 개수 자동 계산\n",
    "# (N / {weather type pool size})\n",
    "N_SAMPLES_PER_CLASS = int(np.ceil(TOTAL_DATASET_SIZE_N / weather_type_pool_size))\n",
    "\n",
    "print(f\"발견된 날씨 코드 종류: {weather_type_pool_size} 개\")\n",
    "print(f\"각 코드별 샘플링 목표 개수: {N_SAMPLES_PER_CLASS} 개 (자동 계산됨)\")\n",
    "\n",
    "def stratified_sample(df, col, n_samples):\n",
    "    \"\"\"\n",
    "    데이터프레임을 'col' 기준으로 그룹화하고, \n",
    "    각 그룹에서 n_samples 개수만큼 샘플링합니다. (그룹 크기가 n_samples보다 작으면 모두 반환)\n",
    "    \"\"\"\n",
    "    return df.groupby(col, group_keys=False).apply(lambda x: x.sample(n=min(len(x), n_samples)))\n",
    "\n",
    "balanced_df = stratified_sample(final_df, 'coco', N_SAMPLES_PER_CLASS)\n",
    "balanced_df = balanced_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n균형 샘플링 완료. 최종 {len(balanced_df)}개의 샘플을 확보했습니다.\")\n",
    "print(\"최종 샘플의 날씨 코드(coco)별 분포:\")\n",
    "print(balanced_df['coco'].value_counts().sort_index())\n",
    "print(\"\\n최종 샘플의 로컬 시간대별 분포:\")\n",
    "print(balanced_df['local_hour'].value_counts().sort_index())\n",
    "\n",
    "# --- 7. CSV 파일로 저장 ---\n",
    "balanced_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"\\n성공! {OUTPUT_FILE} 파일이 저장되었습니다.\")\n",
    "print(f\"저장된 컬럼 목록: {list(balanced_df.columns)}\")\n",
    "print(\"이 CSV 파일을 GEE Asset에 테이블로 업로드하여 사용할 수 있습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74153567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기\n",
    "stations_df = pd.read_csv('active_stations_lat_lon.csv')\n",
    "ground_truth_df = pd.read_csv('gee_weather_ground_truth_balanced.csv')\n",
    "\n",
    "# lat, lon을 기준으로 ID 매핑 딕셔너리 생성\n",
    "# 소수점 정밀도를 고려하여 반올림\n",
    "stations_df['lat_round'] = stations_df['lat'].round(4)\n",
    "stations_df['lon_round'] = stations_df['lon'].round(4)\n",
    "\n",
    "# (lat, lon) -> id 매핑 딕셔너리\n",
    "station_dict = dict(zip(\n",
    "    zip(stations_df['lat_round'], stations_df['lon_round']), \n",
    "    stations_df['id']\n",
    "))\n",
    "\n",
    "# ground_truth에도 반올림된 컬럼 생성\n",
    "ground_truth_df['lat_round'] = ground_truth_df['lat'].round(4)\n",
    "ground_truth_df['lon_round'] = ground_truth_df['lon'].round(4)\n",
    "\n",
    "# ID 매핑\n",
    "ground_truth_df['id'] = ground_truth_df.apply(\n",
    "    lambda row: station_dict.get((row['lat_round'], row['lon_round']), None),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 임시 컬럼 제거\n",
    "ground_truth_df = ground_truth_df.drop(['lat_round', 'lon_round'], axis=1)\n",
    "\n",
    "# 컬럼 순서 재배열 (id를 첫 번째 컬럼으로)\n",
    "cols = ['id'] + [col for col in ground_truth_df.columns if col != 'id']\n",
    "ground_truth_df = ground_truth_df[cols]\n",
    "\n",
    "# 결과 저장\n",
    "ground_truth_df.to_csv('gee_weather_ground_truth_balanced_with_id.csv', index=False)\n",
    "\n",
    "# 매칭 결과 확인\n",
    "print(f\"Total rows: {len(ground_truth_df)}\")\n",
    "print(f\"Matched rows (with ID): {ground_truth_df['id'].notna().sum()}\")\n",
    "print(f\"Unmatched rows (without ID): {ground_truth_df['id'].isna().sum()}\")\n",
    "\n",
    "# 처음 몇 행 출력\n",
    "print(\"\\n첫 5행:\")\n",
    "print(ground_truth_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
